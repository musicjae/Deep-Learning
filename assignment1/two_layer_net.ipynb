{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "name": "two_layer_net.ipynb",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/musicjae/cs231n/blob/master/assignment1/two_layer_net.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SmAQ_KcwRYOE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 496
        },
        "outputId": "299b4e39-f7f9-42b4-c25b-0641dfa1eaf1"
      },
      "source": [
        "# this mounts your Google Drive to the Colab VM.\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "# enter the foldername in your Drive where you have saved the unzipped\n",
        "# assignment folder, e.g. 'cs231n/assignments/assignment3/'\n",
        "FOLDERNAME = 'Colab Notebooks/cs231n/assignments/assignment1'\n",
        "assert FOLDERNAME is not None, \"[!] Enter the foldername.\"\n",
        "\n",
        "# now that we've mounted your Drive, this ensures that\n",
        "# the Python interpreter of the Colab VM can load\n",
        "# python files from within it.\n",
        "import sys\n",
        "sys.path.append('/content/drive/My Drive/{}'.format(FOLDERNAME))\n",
        "\n",
        "# this downloads the CIFAR-10 dataset to your Drive\n",
        "# if it doesn't already exist.\n",
        "%cd drive/My\\ Drive/$FOLDERNAME/cs231n/datasets/\n",
        "!bash get_datasets.sh\n",
        "%cd /content"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n",
            "/content/drive/My Drive/Colab Notebooks/cs231n/assignments/assignment1/cs231n/datasets\n",
            "--2020-07-21 07:28:54--  http://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "Resolving www.cs.toronto.edu (www.cs.toronto.edu)... 128.100.3.30\n",
            "Connecting to www.cs.toronto.edu (www.cs.toronto.edu)|128.100.3.30|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 170498071 (163M) [application/x-gzip]\n",
            "Saving to: ‘cifar-10-python.tar.gz’\n",
            "\n",
            "cifar-10-python.tar 100%[===================>] 162.60M  47.5MB/s    in 3.8s    \n",
            "\n",
            "2020-07-21 07:28:58 (42.8 MB/s) - ‘cifar-10-python.tar.gz’ saved [170498071/170498071]\n",
            "\n",
            "cifar-10-batches-py/\n",
            "cifar-10-batches-py/data_batch_4\n",
            "cifar-10-batches-py/readme.html\n",
            "cifar-10-batches-py/test_batch\n",
            "cifar-10-batches-py/data_batch_3\n",
            "cifar-10-batches-py/batches.meta\n",
            "cifar-10-batches-py/data_batch_2\n",
            "cifar-10-batches-py/data_batch_5\n",
            "cifar-10-batches-py/data_batch_1\n",
            "/content\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [
          "pdf-title"
        ],
        "id": "LTJim0H0RRKj",
        "colab_type": "text"
      },
      "source": [
        "# Implementing a Neural Network\n",
        "In this exercise we will develop a neural network with fully-connected layers to perform classification, and test it out on the CIFAR-10 dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [
          "pdf-ignore"
        ],
        "id": "bHi0LFQrRRKk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# A bit of setup\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from cs231n.classifiers.neural_net import TwoLayerNet\n",
        "\n",
        "%matplotlib inline\n",
        "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
        "plt.rcParams['image.interpolation'] = 'nearest'\n",
        "plt.rcParams['image.cmap'] = 'gray'\n",
        "\n",
        "# for auto-reloading external modules\n",
        "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "\n",
        "def rel_error(x, y):\n",
        "    \"\"\" returns relative error \"\"\"\n",
        "    return np.max(np.abs(x - y) / (np.maximum(1e-8, np.abs(x) + np.abs(y))))"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [
          "pdf-ignore"
        ],
        "id": "TBnymGxCRRKn",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "우리는 신경망의 인스턴스를 보여주기 위해 `cs231n/classifiers/neural_net.py`에서 `TwoLayerNet`를 사용할 것이다. 이 신경망의 매개변수는 그 인스턴스의 변수 `self.params`에 저장된다. 여기서 중요한 것은 string 매개변수 이름과 값이 넘파이 배열이란 것이다. 아래에서, 우리는 toy 자료와 수행을 발전시키기 위해 사용할 toy 모델을 초기화한다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [
          "pdf-ignore"
        ],
        "id": "0YCoPDcBRRKn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create a small net and some toy data to check your implementations.\n",
        "# Note that we set the random seed for repeatable experiments.\n",
        "\n",
        "input_size = 4\n",
        "hidden_size = 10\n",
        "num_classes = 3\n",
        "num_inputs = 5\n",
        "\n",
        "def init_toy_model():\n",
        "    np.random.seed(0)\n",
        "    return TwoLayerNet(input_size, hidden_size, num_classes, std=1e-1)\n",
        "\n",
        "def init_toy_data():\n",
        "    np.random.seed(1)\n",
        "    X = 10 * np.random.randn(num_inputs, input_size)\n",
        "    y = np.array([0, 1, 2, 2, 1])\n",
        "    return X, y\n",
        "\n",
        "net = init_toy_model()\n",
        "X, y = init_toy_data()"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GvMTxRRRRRKq",
        "colab_type": "text"
      },
      "source": [
        "## 1. Foward Pass<br>\n",
        "<h5> 두 개의 층이 완전히 연결된 신경망. 그 신경망의 입력 차원은 N, 은닉층 차원은 H이고, 이것은 C 개의 클래스로 분류를 한다. 우리는 가중치 행렬, 소프트맥스 손실 함수, L2 정규화를 가지고 이 신경망을 훈련시킨다. 이 신경망은 먼저 완전히 연결된 층에 ReLU 비선형성을 사용한다. [*fully connected layer = 완전히 연결된 층.] \n",
        "\n",
        "<br>\n",
        "\n",
        "즉, 이 신경망은 다음의 구조를 갖는다.\n",
        "    \n",
        "<br>\n",
        "\n",
        "입력 -- 완전히 연결된 층 -- ReLU -- 완전히 연결된 층 -- 소프트맥스<Br>  \n",
        "    두 번째 완전히 연결된 층의 아웃풋은 \"각 클래스의 스코어\"이다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a9OK6cQ2RRKq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        },
        "outputId": "744c4743-67f2-4803-dfa8-309ba35e7742"
      },
      "source": [
        "scores = net.loss(X)\n",
        "print('Your scores:')\n",
        "print(scores)\n",
        "print()\n",
        "print('correct scores:')\n",
        "correct_scores = np.asarray([\n",
        "  [-0.81233741, -1.27654624, -0.70335995],\n",
        "  [-0.17129677, -1.18803311, -0.47310444],\n",
        "  [-0.51590475, -1.01354314, -0.8504215 ],\n",
        "  [-0.15419291, -0.48629638, -0.52901952],\n",
        "  [-0.00618733, -0.12435261, -0.15226949]])\n",
        "print(correct_scores)\n",
        "print()\n",
        "\n",
        "# The difference should be very small. We get < 1e-7\n",
        "print('Difference between your scores and correct scores:')\n",
        "print(np.sum(np.abs(scores - correct_scores)))\n",
        "\n",
        "if 1e-7 > np.sum(np.abs(scores - correct_scores)):\n",
        "    print('옳게 프로그래밍 함.')"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Your scores:\n",
            "[[-0.81233741 -1.27654624 -0.70335995]\n",
            " [-0.17129677 -1.18803311 -0.47310444]\n",
            " [-0.51590475 -1.01354314 -0.8504215 ]\n",
            " [-0.15419291 -0.48629638 -0.52901952]\n",
            " [-0.00618733 -0.12435261 -0.15226949]]\n",
            "\n",
            "correct scores:\n",
            "[[-0.81233741 -1.27654624 -0.70335995]\n",
            " [-0.17129677 -1.18803311 -0.47310444]\n",
            " [-0.51590475 -1.01354314 -0.8504215 ]\n",
            " [-0.15419291 -0.48629638 -0.52901952]\n",
            " [-0.00618733 -0.12435261 -0.15226949]]\n",
            "\n",
            "Difference between your scores and correct scores:\n",
            "3.6802720745909845e-08\n",
            "옳게 프로그래밍 함.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IYeUSjYNSnr6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 708
        },
        "outputId": "4f4a1317-aa13-4f32-c555-8200c0254f90"
      },
      "source": [
        "#My studying, not contained.\n",
        "\n",
        "from __future__ import print_function\n",
        "from builtins import range\n",
        "from builtins import object\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from past.builtins import xrange\n",
        "from cs231n.gradient_check import eval_numerical_gradient\n",
        "\n",
        "class TwoLayerNet(object):\n",
        "   \n",
        "    def __init__(self, input_size, hidden_size, output_size, std=1e-4): # \n",
        "        \"\"\"\n",
        "       \n",
        "        모델을 초기화하라. 가중치는 작은 랜덤 값으로 초기화되고, 편향은 0으로 초기화된다. 가중치와 편향은 self.params 변항에 저장된다. \n",
        "        이는 다음의 key를 가진 dictionary이다.\n",
        "\n",
        "        W1: First layer weights; has shape (D, H)\n",
        "        b1: First layer biases; has shape (H,)\n",
        "        \n",
        "        W2: Second layer weights; has shape (H, C)\n",
        "        b2: Second layer biases; has shape (C,)\n",
        "\n",
        "        Inputs:\n",
        "        \n",
        "        - input_size: The dimension D of the input data.\n",
        "        - hidden_size: The number of neurons H in the hidden layer.\n",
        "        - output_size: The number of classes C.\n",
        "        \"\"\"\n",
        "        \n",
        "        #인풋 크기: (인풋 자료의 차원 수) D, 은닉 크기: (뉴런 수) H, 아웃풋 크기: (클래스 개수) C\n",
        "        \n",
        "        self.params = {} # dic 형태인 변항에 가중치, 편향 저장.\n",
        "        self.params['W1'] = std * np.random.randn(input_size, hidden_size) # 가중치 랜덤값으로 초기화\n",
        "        self.params['b1'] = np.zeros(hidden_size) # 편향 항은 0으로 초기화\n",
        "        \n",
        "        self.params['W2'] = std * np.random.randn(hidden_size, output_size)\n",
        "        self.params['b2'] = np.zeros(output_size)\n",
        " \n",
        "    def loss(self, X, y=None, reg=0.0):\n",
        "        \n",
        "        \"\"\"\n",
        "        손실, 그래디언트 계산하라 for a two layer fully connected neural network.\n",
        "\n",
        "        Inputs:\n",
        "        \n",
        "        - X: Input data of shape (N, D). Each X[i] is a training sample.\n",
        "        - y: Vector of training labels. y[i] is the label for X[i], and each y[i] is\n",
        "          an integer in the range 0 <= y[i] < C. This parameter is optional; if it\n",
        "          is not passed then we only return scores, and if it is passed then we\n",
        "          instead return the loss and gradients.\n",
        "        - reg: Regularization strength.\n",
        "\n",
        "        Returns:\n",
        "        \n",
        "        If y is None, return a matrix scores of shape (N, C) where scores[i, c] is\n",
        "        the score for class c on input X[i].\n",
        "\n",
        "        If y is not None, instead return a tuple of:\n",
        "        \n",
        "        - loss: 트레이닝 샘플의 batch의 손실 (data 손실 + 정규화 손실)\n",
        "        - grads: <매개변수 이름>이 <손실 함수에 대한 매개변수의 그래디언트>를 사상map하는 dictionary. \n",
        "                 이 dic은 self.params와 동일한 keys를 가진다.\n",
        "        \"\"\"\n",
        "        \n",
        "        # Unpack variables from the params dictionary\n",
        "        W1, b1 = self.params['W1'], self.params['b1']\n",
        "        W2, b2 = self.params['W2'], self.params['b2']\n",
        "        N, D = X.shape\n",
        "\n",
        "        # Compute the forward pass\n",
        "        s = None # The s is a score\n",
        "\n",
        "        # TODO: the forward pass를 수행하라, 인풋의 클래스 스코어를 계산하라\n",
        "        # 스코어 변항에 그 결과를 저장하라. 그것은 shape (N, C)인 배열이어야 한다.  \n",
        "        \n",
        "        # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "        \n",
        "        f = np.dot(X, W1) + b1\n",
        "        h= np.maximum(0, f) # ReLU 함수화\n",
        "        \n",
        "        s = np.dot(h, W2) + b2 # 위 그림 참고\n",
        "        \n",
        "        \n",
        "        # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "\n",
        "        # If the targets are not given then jump out, we're done\n",
        "        if y is None:\n",
        "            return s # (N, C)\n",
        "\n",
        "        # Compute the loss\n",
        "        \n",
        "        loss = None # 초기화\n",
        "        \n",
        "        #############################################################################\n",
        "        # TODO: forward pass를 완성하고, the loss를 계산하라. 이것은 W1, W2에 대한 data 손실과 \n",
        "        # 정규화를 포함해야 한다. 그리고 그 결과를 loss의 변항에 저장하라. 이는 스칼라여야 한다.       \n",
        "        # 소프트맥스 손실 분류기를 사용하라           \n",
        "        #                   \n",
        "        #############################################################################\n",
        "        # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "\n",
        "        s -= np.max(s)\n",
        "        exp_s = np.exp(s)\n",
        "        sum_s = np.sum(exp_s, axis= -1, keepdims= True) #softmax 때와 같이, 벡터화 해주기\n",
        "        out = exp_s/sum_s\n",
        "        \n",
        "        loss = -np.sum(np.log(out[np.arange(N), y]))\n",
        "        \n",
        "        loss /= N\n",
        "        loss += 0.5 *reg * (np.sum(W1**2) + np.sum(W2**2)) #가중치가 2 개니까 각각 정규화한 뒤 더한다\n",
        "        \n",
        "        # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "        \n",
        "        # gradiet 구하기\n",
        "        \n",
        "        grads = {}\n",
        "        \n",
        "        \"\"\"\n",
        "        To do: 가중치와 bias를 미분하는 Backward pass 를 계산하라. \n",
        "               그 결과를 grad 딕셔너리 안에 저장하라. 가령, grad['W1']은 W1의\n",
        "               그래디언트를 저장해야 하고, 이 둘의 크기는 같아야 한다.\n",
        "        \"\"\"\n",
        "        s = 0 # The s is a score\n",
        "        f = np.dot(X, W1) + b1\n",
        "        h = np.maximum(0, f) # maximum(): Element-wise maximum of array elements.\n",
        "        \n",
        "        s = np.dot(h, W2) + b2 # 위 그림 참고\n",
        "        \n",
        "        s -= np.max(s)\n",
        "        exp_s = np.exp(s)\n",
        "        sum_s = np.sum(exp_s, axis= -1, keepdims= True) #softmax 때와 같이, 벡터화 해주기\n",
        "        out = exp_s/sum_s# output을 구하기 위해, 위와 동일한 작업 수행\n",
        "        \n",
        "        #Backpropagation\n",
        "        \n",
        "        ds = out\n",
        "        ds[np.arange(N),y] -= 1 # 소프트맥스에 대한 backpropagation\n",
        "        \n",
        "        dh = np.dot(ds, W2.T) # s = W2*h + b2 이니까, dh에 대한 backpropagation\n",
        "        \n",
        "        dh[h <= 0] = 0 # ReLU에 대한 backpropagation\n",
        "        df = dh\n",
        "        \n",
        "        # 매개변수의 그래디언트 계산하기\n",
        "        \n",
        "        grads['W2'] = np.dot(h.T, ds) \n",
        "        grads['W1'] = np.dot(X.T, df) \n",
        "        grads['b2'] = np.sum(ds, axis=0)\n",
        "        grads['b1'] = np.sum(df, axis=0)\n",
        "        \n",
        "\n",
        "          # 매개변수의 그래디언트 계산하기\n",
        "        \n",
        "        dW2 = np.dot(h.T, ds) # [HxN] * [NxC] = [HxC]\n",
        "        dW1 = np.dot(X.T, df) # [DxN] * [NxH] = [DxH]\n",
        "        db2 = np.sum(ds, axis=0)\n",
        "        db1 = np.sum(df, axis=0) # db1 = np.ones((1, num_train)).dot(hidden)\n",
        "        \n",
        "        dW2 /= N\n",
        "        dW1 /= N\n",
        "        db2 /= N\n",
        "        db1 /= N \n",
        "        \n",
        "\n",
        "        # 정규화\n",
        "      \n",
        "        dW1 += reg*W1\n",
        "        dW2 += reg*W2\n",
        "        \n",
        "        grads = {'W1':dW1, 'W2' : dW2, 'b1':db1, 'b2': db2}\n",
        "        \n",
        "        return loss, grads\n",
        "\"\"\"\"\n",
        "우리는 신경망의 인스턴스를 보여주기 위해 바로 위에서 작성한 `TwoLayerNet`를 사용할 것이다. \n",
        "이 신경망의 매개변수는 그 인스턴스의 변수 `self.params`에 저장된다. 여기서 중요한 것은 string 매개변수 names와 values가 둘다 배열이란 것이다.\n",
        "아래에서, 우리는 toy의 자료와 implementaion을 발전시키기 위해 사용할 <toy 모델>을 초기화한다.\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "\n",
        "input_size = 4\n",
        "hidden_size = 10\n",
        "num_classes = 3\n",
        "num_inputs = 5\n",
        "\n",
        "def init_toy_model():\n",
        "    np.random.seed(0)\n",
        "    return TwoLayerNet(input_size, hidden_size, num_classes, std=1e-1)\n",
        "\n",
        "def init_toy_data():\n",
        "    np.random.seed(1)\n",
        "    X = 10 * np.random.randn(num_inputs, input_size) # X(5,4)\n",
        "    y = np.array([0, 1, 2, 2, 1]) # y(5,)\n",
        "    return X, y \n",
        "\n",
        "net = init_toy_model()\n",
        "X, y = init_toy_data()\n",
        "\n",
        "\"\"\"\n",
        "Forward pass: 스코어 계산하기\n",
        "`cs231n/classifiers/neural_net.py` 파일의 TwoLayerNet.loss 메서드를 보라. \n",
        "이 함수는 SVM, Softmax 예제에서 작성했던 손실 함수와 매우 비슷하다: 이 함수는 data와 가중치를 취하고, 클래스 스코어, 손실, 그래디언트를 계산한다.\n",
        "모든 입력의 스코어를 계산하기 위해 가중치와 편향을 사용하는 forward pass의 첫 번째 부분을 실행해보자.\n",
        "\"\"\"\n",
        "\n",
        "scores = net.loss(X)\n",
        "print(' 1. Your scores:')\n",
        "print(scores)\n",
        "print()\n",
        "print(' 2. correct scores:')\n",
        "correct_scores = np.asarray([\n",
        "  [-0.81233741, -1.27654624, -0.70335995],\n",
        "  [-0.17129677, -1.18803311, -0.47310444],\n",
        "  [-0.51590475, -1.01354314, -0.8504215 ],\n",
        "  [-0.15419291, -0.48629638, -0.52901952],\n",
        "  [-0.00618733, -0.12435261, -0.15226949]])\n",
        "print(correct_scores)\n",
        "print()\n",
        "\n",
        "# The difference should be very small. We get < 1e-7\n",
        "print(' 3. 나의 스코어와 정답 스코어 간 차이: ')\n",
        "print('%e, %f' % (np.sum(np.abs(scores - correct_scores)),np.sum(np.abs(scores - correct_scores))))\n",
        "\n",
        "# In the same function, implement the second part that computes the data and regularization loss.\n",
        "print('\\n')\n",
        "\n",
        "loss = net.loss(X, y, reg=0.05)\n",
        "correct_loss = 1.30378789133\n",
        "\n",
        "# should be very small, we get < 1e-12\n",
        "print(' 4. 나의 손실과 정답 손실 간 차이: ')\n",
        "\n",
        "print('%f' % np.sum(np.abs(loss[0] - correct_loss)))\n",
        "\n",
        "#그래디언트 체킹\n",
        "\n",
        "loss, grads = net.loss(X, y, reg=0.05)\n",
        "\n",
        "#loss, grads = net.loss(X, y, reg=0.05)\n",
        "# these should all be less than 1e-8 or so\n",
        "for param_name in grads:\n",
        "    f = lambda W: net.loss(X, y, reg=0.05)\n",
        "    param_grad_num = eval_numerical_gradient(f, net.params[param_name], verbose=False)\n",
        "    print('4. %s max relative error: %e' % ( rel_error(param_grad_num, grads[param_name])))\n",
        "    \n",
        "   "
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " 1. Your scores:\n",
            "[[-0.81233741 -1.27654624 -0.70335995]\n",
            " [-0.17129677 -1.18803311 -0.47310444]\n",
            " [-0.51590475 -1.01354314 -0.8504215 ]\n",
            " [-0.15419291 -0.48629638 -0.52901952]\n",
            " [-0.00618733 -0.12435261 -0.15226949]]\n",
            "\n",
            " 2. correct scores:\n",
            "[[-0.81233741 -1.27654624 -0.70335995]\n",
            " [-0.17129677 -1.18803311 -0.47310444]\n",
            " [-0.51590475 -1.01354314 -0.8504215 ]\n",
            " [-0.15419291 -0.48629638 -0.52901952]\n",
            " [-0.00618733 -0.12435261 -0.15226949]]\n",
            "\n",
            " 3. 나의 스코어와 정답 스코어 간 차이: \n",
            "3.680272e-08, 0.000000\n",
            "\n",
            "\n",
            " 4. 나의 손실과 정답 손실 간 차이: \n",
            "0.018965\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-eab5d9efe086>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    242\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mparam_name\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgrads\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m     \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mW\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreg\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.05\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 244\u001b[0;31m     \u001b[0mparam_grad_num\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meval_numerical_gradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mparam_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    245\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'4. %s max relative error: %e'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m \u001b[0mrel_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam_grad_num\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrads\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mparam_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/drive/My Drive/Colab Notebooks/cs231n/assignments/assignment1/cs231n/gradient_check.py\u001b[0m in \u001b[0;36meval_numerical_gradient\u001b[0;34m(f, x, verbose, h)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0;31m# compute the partial derivative with centered formula\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m         \u001b[0mgrad\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mix\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mfxph\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mfxmh\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# the slope\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mix\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for -: 'tuple' and 'tuple'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1A-yfujnRRKt",
        "colab_type": "text"
      },
      "source": [
        "# Forward pass: compute loss\n",
        "In the same function, implement the second part that computes the data and regularization loss."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l9guzBNlRRKt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "9c639c25-6455-4f53-d3ff-e4c52475f538"
      },
      "source": [
        "\n",
        "loss, _ = net.loss(X, y, reg=0.05)\n",
        "correct_loss = 1.30378789133\n",
        "\n",
        "# should be very small, we get < 1e-12\n",
        "print('Difference between your loss and correct loss:')\n",
        "print(np.sum(np.abs(loss - correct_loss)))\n",
        "\n",
        "if 1e-12 > np.sum(np.abs(loss - correct_loss)):\n",
        "    print('옳게 프로그래밍 함')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Difference between your loss and correct loss:\n",
            "1.7985612998927536e-13\n",
            "옳게 프로그래밍 함\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ydg0fzMWRRKv",
        "colab_type": "text"
      },
      "source": [
        "## 2. Backward pass<br>\n",
        "##### 남은 함수들을 작성해보라. 이 함수는 변수 'W1', 'b1', W2', 그리고 'b2'에 대한 손실의 그래디언트를 계산할 것이다. 네가 forward pass를 올바르게 작성했다면, 너는 수치적 그래디언트를 사용하여 backward pass를 디버그할 수 있다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ECdGmjCdRRKw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "outputId": "bdd81c10-68d9-4ae3-8e94-192d92aa86bc"
      },
      "source": [
        "from cs231n.gradient_check import eval_numerical_gradient\n",
        "\n",
        "# Use numeric gradient checking to check your implementation of the backward pass.\n",
        "# If your implementation is correct, the difference between the numeric and\n",
        "# analytic gradients should be less than 1e-8 for each of W1, W2, b1, and b2.\n",
        "loss, grads = net.loss(X, y, reg=0.05)\n",
        "\n",
        "\n",
        "# these should all be less than 1e-8 or so\n",
        "for param_name in grads:\n",
        "    f = lambda W: net.loss(X, y, reg=0.05)[0]\n",
        "    param_grad_num = eval_numerical_gradient(f, net.params[param_name], verbose=False)\n",
        "    print('%s max relative error: %e' % (param_name, rel_error(param_grad_num, grads[param_name])))\n",
        "    print('=%f' % ( rel_error(param_grad_num, grads[param_name])))\n",
        "    if (rel_error(param_grad_num, grads[param_name])) < 1e-8:\n",
        "        print('\\n >>>> correct')"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W1 max relative error: 1.000000e+00\n",
            "=1.000000\n",
            "W2 max relative error: 3.440708e-09\n",
            "=0.000000\n",
            "\n",
            " >>>> correct\n",
            "b1 max relative error: 1.000000e+00\n",
            "=1.000000\n",
            "b2 max relative error: 4.447625e-11\n",
            "=0.000000\n",
            "\n",
            " >>>> correct\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "98xvdbjZRRKz",
        "colab_type": "text"
      },
      "source": [
        "# Train the network\n",
        "To train the network we will use stochastic gradient descent (SGD), similar to the SVM and Softmax classifiers. Look at the function `TwoLayerNet.train` and fill in the missing sections to implement the training procedure. This should be very similar to the training procedure you used for the SVM and Softmax classifiers. You will also have to implement `TwoLayerNet.predict`, as the training process periodically performs prediction to keep track of accuracy over time while the network trains.\n",
        "\n",
        "Once you have implemented the method, run the code below to train a two-layer network on toy data. You should achieve a training loss less than 0.02."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "final_training_loss",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 530
        },
        "outputId": "cc34d351-3920-43e7-d2f5-e4d887478bdd"
      },
      "source": [
        "net = init_toy_model()\n",
        "stats = net.train(X, y, X, y,\n",
        "            learning_rate=1e-1, reg=5e-6,\n",
        "            num_iters=100, verbose=False)\n",
        "\n",
        "print('Final training loss: ', stats['loss_history'][-1])\n",
        "\n",
        "# plot the loss history\n",
        "plt.plot(stats['loss_history'])\n",
        "plt.xlabel('iteration')\n",
        "plt.ylabel('training loss')\n",
        "plt.title('Training Loss history')\n",
        "plt.show()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Final training loss:  1.0681514378278365\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x576 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CFmAA_tmRRK2",
        "colab_type": "text"
      },
      "source": [
        "# Load the data\n",
        "Now that you have implemented a two-layer network that passes gradient checks and works on toy data, it's time to load up our favorite CIFAR-10 data so we can use it to train a classifier on a real dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [
          "pdf-ignore"
        ],
        "id": "hhT4FV_mRRK3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "4b5c9fbd-8e11-49bd-bb21-30bf5f911253"
      },
      "source": [
        "from cs231n.data_utils import load_CIFAR10\n",
        "\n",
        "def get_CIFAR10_data(num_training=49000, num_validation=1000, num_test=1000):\n",
        "    \"\"\"\n",
        "    Load the CIFAR-10 dataset from disk and perform preprocessing to prepare\n",
        "    it for the two-layer neural net classifier. These are the same steps as\n",
        "    we used for the SVM, but condensed to a single function.  \n",
        "    \"\"\"\n",
        "    # Load the raw CIFAR-10 data\n",
        "    cifar10_dir = '/content/drive/My Drive/Colab Notebooks/cs231n/cifar-10-batches-py'\n",
        "    \n",
        "    # Cleaning up variables to prevent loading data multiple times (which may cause memory issue)\n",
        "    try:\n",
        "       del X_train, y_train\n",
        "       del X_test, y_test\n",
        "       print('Clear previously loaded data.')\n",
        "    except:\n",
        "       pass\n",
        "\n",
        "    X_train, y_train, X_test, y_test = load_CIFAR10(cifar10_dir)\n",
        "        \n",
        "    # Subsample the data\n",
        "    mask = list(range(num_training, num_training + num_validation))\n",
        "    X_val = X_train[mask]\n",
        "    y_val = y_train[mask]\n",
        "    mask = list(range(num_training))\n",
        "    X_train = X_train[mask]\n",
        "    y_train = y_train[mask]\n",
        "    mask = list(range(num_test))\n",
        "    X_test = X_test[mask]\n",
        "    y_test = y_test[mask]\n",
        "\n",
        "    # Normalize the data: subtract the mean image\n",
        "    mean_image = np.mean(X_train, axis=0)\n",
        "    X_train -= mean_image\n",
        "    X_val -= mean_image\n",
        "    X_test -= mean_image\n",
        "\n",
        "    # Reshape data to rows\n",
        "    X_train = X_train.reshape(num_training, -1)\n",
        "    X_val = X_val.reshape(num_validation, -1)\n",
        "    X_test = X_test.reshape(num_test, -1)\n",
        "\n",
        "    return X_train, y_train, X_val, y_val, X_test, y_test\n",
        "\n",
        "\n",
        "# Invoke the above function to get our data.\n",
        "X_train, y_train, X_val, y_val, X_test, y_test = get_CIFAR10_data()\n",
        "print('Train data shape: ', X_train.shape)\n",
        "print('Train labels shape: ', y_train.shape)\n",
        "print('Validation data shape: ', X_val.shape)\n",
        "print('Validation labels shape: ', y_val.shape)\n",
        "print('Test data shape: ', X_test.shape)\n",
        "print('Test labels shape: ', y_test.shape)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train data shape:  (49000, 3072)\n",
            "Train labels shape:  (49000,)\n",
            "Validation data shape:  (1000, 3072)\n",
            "Validation labels shape:  (1000,)\n",
            "Test data shape:  (1000, 3072)\n",
            "Test labels shape:  (1000,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HGylmnwTRRK6",
        "colab_type": "text"
      },
      "source": [
        "# Train a network\n",
        "To train our network we will use SGD. In addition, we will adjust the learning rate with an exponential learning rate schedule as optimization proceeds; after each epoch, we will reduce the learning rate by multiplying it by a decay rate."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [
          "code"
        ],
        "id": "OAaQE3z1RRK6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "input_size = 32 * 32 * 3\n",
        "hidden_size = 50\n",
        "num_classes = 10\n",
        "net = TwoLayerNet(input_size, hidden_size, num_classes)\n",
        "\n",
        "# Train the network\n",
        "stats = net.train(X_train, y_train, X_val, y_val,\n",
        "            num_iters=1000, batch_size=200,\n",
        "            learning_rate=1e-4, learning_rate_decay=0.95,\n",
        "            reg=0.25, verbose=True)\n",
        "\n",
        "# Predict on the validation set\n",
        "val_acc = (net.predict(X_val) == y_val).mean()\n",
        "print('Validation accuracy: ', val_acc)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yuJBs3dXRRK8",
        "colab_type": "text"
      },
      "source": [
        "# Debug the training\n",
        "With the default parameters we provided above, you should get a validation accuracy of about 0.29 on the validation set. This isn't very good.\n",
        "\n",
        "One strategy for getting insight into what's wrong is to plot the loss function and the accuracies on the training and validation sets during optimization.\n",
        "\n",
        "Another strategy is to visualize the weights that were learned in the first layer of the network. In most neural networks trained on visual data, the first layer weights typically show some visible structure when visualized."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1eX9BGUDRRK9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Plot the loss function and train / validation accuracies\n",
        "plt.subplot(2, 1, 1)\n",
        "plt.plot(stats['loss_history'])\n",
        "plt.title('Loss history')\n",
        "plt.xlabel('Iteration')\n",
        "plt.ylabel('Loss')\n",
        "\n",
        "plt.subplot(2, 1, 2)\n",
        "plt.plot(stats['train_acc_history'], label='train')\n",
        "plt.plot(stats['val_acc_history'], label='val')\n",
        "plt.title('Classification accuracy history')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Classification accuracy')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JblXQBrERRK_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from cs231n.vis_utils import visualize_grid\n",
        "\n",
        "# Visualize the weights of the network\n",
        "\n",
        "def show_net_weights(net):\n",
        "    W1 = net.params['W1']\n",
        "    W1 = W1.reshape(32, 32, 3, -1).transpose(3, 0, 1, 2)\n",
        "    plt.imshow(visualize_grid(W1, padding=3).astype('uint8'))\n",
        "    plt.gca().axis('off')\n",
        "    plt.show()\n",
        "\n",
        "show_net_weights(net)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TJKr7x3fRRLC",
        "colab_type": "text"
      },
      "source": [
        "# Tune your hyperparameters\n",
        "\n",
        "**What's wrong?**. Looking at the visualizations above, we see that the loss is decreasing more or less linearly, which seems to suggest that the learning rate may be too low. Moreover, there is no gap between the training and validation accuracy, suggesting that the model we used has low capacity, and that we should increase its size. On the other hand, with a very large model we would expect to see more overfitting, which would manifest itself as a very large gap between the training and validation accuracy.\n",
        "\n",
        "**Tuning**. Tuning the hyperparameters and developing intuition for how they affect the final performance is a large part of using Neural Networks, so we want you to get a lot of practice. Below, you should experiment with different values of the various hyperparameters, including hidden layer size, learning rate, numer of training epochs, and regularization strength. You might also consider tuning the learning rate decay, but you should be able to get good performance using the default value.\n",
        "\n",
        "**Approximate results**. You should be aim to achieve a classification accuracy of greater than 48% on the validation set. Our best network gets over 52% on the validation set.\n",
        "\n",
        "**Experiment**: You goal in this exercise is to get as good of a result on CIFAR-10 as you can (52% could serve as a reference), with a fully-connected Neural Network. Feel free implement your own techniques (e.g. PCA to reduce dimensionality, or adding dropout, or adding features to the solver, etc.)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [
          "pdf-inline"
        ],
        "id": "zQIDc_HpRRLD",
        "colab_type": "text"
      },
      "source": [
        "**Explain your hyperparameter tuning process below.**\n",
        "\n",
        "$\\color{blue}{\\textit Your Answer:}$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [
          "code"
        ],
        "id": "z8CAAK8xRRLD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "best_net = None # store the best model into this \n",
        "\n",
        "#################################################################################\n",
        "# TODO: Tune hyperparameters using the validation set. Store your best trained  #\n",
        "# model in best_net.                                                            #\n",
        "#                                                                               #\n",
        "# To help debug your network, it may help to use visualizations similar to the  #\n",
        "# ones we used above; these visualizations will have significant qualitative    #\n",
        "# differences from the ones we saw above for the poorly tuned network.          #\n",
        "#                                                                               #\n",
        "# Tweaking hyperparameters by hand can be fun, but you might find it useful to  #\n",
        "# write code to sweep through possible combinations of hyperparameters          #\n",
        "# automatically like we did on the previous exercises.                          #\n",
        "#################################################################################\n",
        "# *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "\n",
        "pass\n",
        "\n",
        "# *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "val_accuracy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Print your validation accuracy: this should be above 48%\n",
        "val_acc = (best_net.predict(X_val) == y_val).mean()\n",
        "print('Validation accuracy: ', val_acc)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R7qvAf_9RRLH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Visualize the weights of the best network\n",
        "show_net_weights(best_net)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GqL2ZjQ9RRLK",
        "colab_type": "text"
      },
      "source": [
        "# Run on the test set\n",
        "When you are done experimenting, you should evaluate your final trained network on the test set; you should get above 48%."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "test_accuracy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Print your test accuracy: this should be above 48%\n",
        "test_acc = (best_net.predict(X_test) == y_test).mean()\n",
        "print('Test accuracy: ', test_acc)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [
          "pdf-inline"
        ],
        "id": "koKt8ShcRRLM",
        "colab_type": "text"
      },
      "source": [
        "**Inline Question**\n",
        "\n",
        "Now that you have trained a Neural Network classifier, you may find that your testing accuracy is much lower than the training accuracy. In what ways can we decrease this gap? Select all that apply.\n",
        "\n",
        "1. Train on a larger dataset.\n",
        "2. Add more hidden units.\n",
        "3. Increase the regularization strength.\n",
        "4. None of the above.\n",
        "\n",
        "$\\color{blue}{\\textit Your Answer:}$\n",
        "\n",
        "$\\color{blue}{\\textit Your Explanation:}$\n",
        "\n"
      ]
    }
  ]
}