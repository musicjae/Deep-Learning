{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Two Layer Net<br>\n",
    "#### we will develop a neural network with fully-connected layers to perform classification, and test it out on the CIFAR-10 dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import Image\n",
    "from cs231n.classifiers.neural_net import TwoLayerNet\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "# for auto-reloading external modules\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "def rel_error(x, y):\n",
    "    \"\"\" returns relative error \"\"\"\n",
    "    return np.max(np.abs(x - y) / (np.maximum(1e-8, np.abs(x) + np.abs(y))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Foward Pass<br>\n",
    "#### 두 개의 층이 완전히 연결된 신경망. 그 신경망의 입력 차원은 N, 은닉층 차원은 H이고, 이것은 C 개의 클래스로 분류를 한다. 우리는 가중치 행렬에 소프트맥스 손실 함수, L2 정규화를 가지고 이 신경망을 훈련시킨다. 이 신경망은 먼저 완전히 연결된 층 뒤에 ReLU 비선형성을 사용한다. [*fully connected layer = 완전히 연결된 층.] \n",
    "\n",
    "<br>\n",
    "\n",
    "#### 즉, 이 신경망은 다음의 구조를 갖는다.\n",
    "    \n",
    "<br>\n",
    "\n",
    "#### 입력 -- 완전히 연결된 층 -- ReLU -- 완전히 연결된 층 -- 소프트맥스<Br>  \n",
    "    두 번째 완전히 연결된 층의 아웃풋은 <각 클래스의 스코어>이다.\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](image_two_layer1.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1. Your scores:\n",
      "[[-0.81233741 -1.27654624 -0.70335995]\n",
      " [-0.17129677 -1.18803311 -0.47310444]\n",
      " [-0.51590475 -1.01354314 -0.8504215 ]\n",
      " [-0.15419291 -0.48629638 -0.52901952]\n",
      " [-0.00618733 -0.12435261 -0.15226949]]\n",
      "\n",
      " 2. correct scores:\n",
      "[[-0.81233741 -1.27654624 -0.70335995]\n",
      " [-0.17129677 -1.18803311 -0.47310444]\n",
      " [-0.51590475 -1.01354314 -0.8504215 ]\n",
      " [-0.15419291 -0.48629638 -0.52901952]\n",
      " [-0.00618733 -0.12435261 -0.15226949]]\n",
      "\n",
      " 3. 나의 스코어와 정답 스코어 간 차이: \n",
      "3.680272e-08, 0.000000\n",
      "\n",
      "\n",
      " 4. 나의 손실과 정답 손실 간 차이: \n",
      "0.018965\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "from builtins import range\n",
    "from builtins import object\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from past.builtins import xrange\n",
    "\n",
    "class TwoLayerNet(object):\n",
    "   \n",
    "    def __init__(self, input_size, hidden_size, output_size, std=1e-4): # \n",
    "        \"\"\"\n",
    "       \n",
    "        모델을 초기화하라. 가중치는 작은 랜덤 값으로 초기화되고, 편향은 0으로 초기화된다. 가중치와 편향은 self.params 변항에 저장된다. \n",
    "        이는 다음의 key를 가진 dictionary이다.\n",
    "\n",
    "        W1: First layer weights; has shape (D, H)\n",
    "        b1: First layer biases; has shape (H,)\n",
    "        \n",
    "        W2: Second layer weights; has shape (H, C)\n",
    "        b2: Second layer biases; has shape (C,)\n",
    "\n",
    "        Inputs:\n",
    "        \n",
    "        - input_size: The dimension D of the input data.\n",
    "        - hidden_size: The number of neurons H in the hidden layer.\n",
    "        - output_size: The number of classes C.\n",
    "        \"\"\"\n",
    "        \n",
    "        #인풋 크기: (인풋 자료의 차원 수) D, 은닉 크기: (뉴런 수) H, 아웃풋 크기: (클래스 개수) C\n",
    "        \n",
    "        self.params = {} # dic 형태인 변항에 가중치, 편향 저장.\n",
    "        self.params['W1'] = std * np.random.randn(input_size, hidden_size) # 가중치 랜덤값으로 초기화\n",
    "        self.params['b1'] = np.zeros(hidden_size) # 편향 항은 0으로 초기화\n",
    "        \n",
    "        self.params['W2'] = std * np.random.randn(hidden_size, output_size)\n",
    "        self.params['b2'] = np.zeros(output_size)\n",
    "\n",
    "    def loss(self, X, y=None, reg=0.0):\n",
    "        \n",
    "        \"\"\"\n",
    "        손실, 그래디언트 계산하라 for a two layer fully connected neural network.\n",
    "\n",
    "        Inputs:\n",
    "        - X: Input data of shape (N, D). Each X[i] is a training sample.\n",
    "        - y: Vector of training labels. y[i] is the label for X[i], and each y[i] is\n",
    "          an integer in the range 0 <= y[i] < C. This parameter is optional; if it\n",
    "          is not passed then we only return scores, and if it is passed then we\n",
    "          instead return the loss and gradients.\n",
    "        - reg: Regularization strength.\n",
    "\n",
    "        Returns:\n",
    "        \n",
    "        If y is None, return a matrix scores of shape (N, C) where scores[i, c] is\n",
    "        the score for class c on input X[i].\n",
    "\n",
    "        If y is not None, instead return a tuple of:\n",
    "        - loss: Loss (data loss and regularization loss) for this batch of training\n",
    "          samples.\n",
    "        - grads: Dictionary mapping parameter names to gradients of those parameters\n",
    "          with respect to the loss function; has the same keys as self.params.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Unpack variables from the params dictionary\n",
    "        W1, b1 = self.params['W1'], self.params['b1']\n",
    "        W2, b2 = self.params['W2'], self.params['b2']\n",
    "        N, D = X.shape\n",
    "\n",
    "        # Compute the forward pass\n",
    "        scores = None\n",
    "\n",
    "        # TODO: the forward pass를 수행하라, 인풋의 클래스 스코어를 계산하라\n",
    "        # 스코어 변항에 그 결과를 저장하라. 그것은 shape (N, C)인 배열이어야 한다.  \n",
    "        \n",
    "        # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "        \n",
    "        h = np.dot(X, W1) + b1\n",
    "        h = np.maximum(0, h) # maximum(): Element-wise maximum of array elements.\n",
    "        \n",
    "        scores = np.dot(h, W2) + b2\n",
    "        \n",
    "        \n",
    "        # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n",
    "        # If the targets are not given then jump out, we're done\n",
    "        if y is None:\n",
    "            return scores\n",
    "\n",
    "        # Compute the loss\n",
    "        loss = 0\n",
    "        #############################################################################\n",
    "        # TODO: forward pass를 완료하고, the loss를 계산하라. 이것은 W1, W2에 대한 data 손실과 \n",
    "        # 정규화를 포함해야 한다. 그리고 그 결과를 loss의 변항에 저장하라. 이는 스칼라여야 한다.       \n",
    "        # 소프트맥스 손실 분류기를 사용하라           \n",
    "        #                   \n",
    "        #############################################################################\n",
    "        # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n",
    "        scores -= np.max(scores)\n",
    "        exp_s = np.exp(scores)\n",
    "        sum_s = np.sum(exp_s, axis= -1, keepdims= True) #softmax 때와 같이, 벡터화 해주기\n",
    "        p = exp_s/sum_s\n",
    "        \n",
    "        loss -= np.sum(np.log(p[np.arange(N), y]))\n",
    "        \n",
    "        loss /= N\n",
    "        loss += 0.5 *reg * (np.sum(W1**2) + np.sum(W2**2)) #가중치가 2 개니까 각각 정규화한 뒤 더한다\n",
    "        \n",
    "        # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "    \n",
    "\n",
    "        return loss\n",
    "\"\"\"\n",
    "\n",
    "우리는 신경망의 인스턴스를 보여주기 위해 바로 위에서 작성한 `TwoLayerNet`를 사용할 것이다. \n",
    "이 신경망의 매개변수는 그 인스턴스의 변수 `self.params`에 저장된다. 여기서 중요한 것은 string 매개변수 names와 values가 둘다 배열이란 것이다.\n",
    "아래에서, 우리는 toy의 자료와 implementaion을 발전시키기 위해 사용할 <toy 모델>을 초기화한다.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "input_size = 4\n",
    "hidden_size = 10\n",
    "num_classes = 3\n",
    "num_inputs = 5\n",
    "\n",
    "def init_toy_model():\n",
    "    np.random.seed(0)\n",
    "    return TwoLayerNet(input_size, hidden_size, num_classes, std=1e-1)\n",
    "\n",
    "def init_toy_data():\n",
    "    np.random.seed(1)\n",
    "    X = 10 * np.random.randn(num_inputs, input_size)\n",
    "    y = np.array([0, 1, 2, 2, 1])\n",
    "    return X, y\n",
    "\n",
    "net = init_toy_model()\n",
    "X, y = init_toy_data()\n",
    "\n",
    "\"\"\"\n",
    "Forward pass: 스코어 계산하기\n",
    "`cs231n/classifiers/neural_net.py` 파일의 TwoLayerNet.loss 메서드를 보라. \n",
    "이 함수는 SVM, Softmax 예제에서 작성했던 손실 함수와 매우 비슷하다: 이 함수는 data와 가중치를 취하고, 클래스 스코어, 손실, 그래디언트를 계산한다.\n",
    "모든 입력의 스코어를 계산하기 위해 가중치와 편향을 사용하는 forward pass의 첫 번째 부분을 실행해보자.\n",
    "\"\"\"\n",
    "\n",
    "scores = net.loss(X)\n",
    "print(' 1. Your scores:')\n",
    "print(scores)\n",
    "print()\n",
    "print(' 2. correct scores:')\n",
    "correct_scores = np.asarray([\n",
    "  [-0.81233741, -1.27654624, -0.70335995],\n",
    "  [-0.17129677, -1.18803311, -0.47310444],\n",
    "  [-0.51590475, -1.01354314, -0.8504215 ],\n",
    "  [-0.15419291, -0.48629638, -0.52901952],\n",
    "  [-0.00618733, -0.12435261, -0.15226949]])\n",
    "print(correct_scores)\n",
    "print()\n",
    "\n",
    "# The difference should be very small. We get < 1e-7\n",
    "print(' 3. 나의 스코어와 정답 스코어 간 차이: ')\n",
    "print('%e, %f' % (np.sum(np.abs(scores - correct_scores)),np.sum(np.abs(scores - correct_scores))))\n",
    "\n",
    "# In the same function, implement the second part that computes the data and regularization loss.\n",
    "print('\\n')\n",
    "loss = net.loss(X, y, reg=0.05)\n",
    "correct_loss = 1.30378789133\n",
    "\n",
    "# should be very small, we get < 1e-12\n",
    "print(' 4. 나의 손실과 정답 손실 간 차이: ')\n",
    "\n",
    "print('%f' % np.sum(np.abs(loss - correct_loss)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Backward pass<br>\n",
    "#### 남은 함수들을 작성해보라. 이 함수는 변수 'W1', 'b1', W2', 그리고 'b2'에 대한 손실의 그래디언트를 계산할 것이다. 네가 forward pass를 올바르게 작성했다면, 너는 수치적 그래디언트를 사용하여 backward pass를 디버그할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
